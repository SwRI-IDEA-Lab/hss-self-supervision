{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2572238e-27d0-4e97-8bb4-de5f5bb9bd23",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ce1e6-0485-4cae-8fdf-163b6a2812d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from lightly.loss import NegativeCosineSimilarity, NTXentLoss\n",
    "from lightly.models.modules.heads import SimSiamPredictionHead, SimSiamProjectionHead\n",
    "\n",
    "from sklearn import random_projection\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "import matplotlib.offsetbox as osb\n",
    "from matplotlib import rcParams as rcp\n",
    "\n",
    "from cuml import UMAP\n",
    "\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "from data.dataset import SDOTilesDataset\n",
    "from data.augmentation_list import AugmentationList\n",
    "\n",
    "seed = 42  # So clever.\n",
    "pl.seed_everything(seed, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296a6ea",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8c1b6",
   "metadata": {},
   "source": [
    "### Define augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a343179",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list = AugmentationList('euv')\n",
    "augmentation_list.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list.keys = ['h_flip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54742957",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78347940",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list.randomize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d90cbb-f8d9-4af7-aadb-ceebfc1c937b",
   "metadata": {},
   "source": [
    "### Initialize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489a6b9-5603-4df7-9c60-0dd399870b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STRIDE = 1\n",
    "DATA_PATH = '/home/jovyan/scratch_space/andresmj/hss-self-supervision/AIA_211_193_171_128x128_small'\n",
    "DATA_PATH = '/d0/euv/aia/preprocessed_ext/AIA_211_193_171/AIA_211_193_171_256x256'\n",
    "DATA_STRIDE = 10000\n",
    "dataset = SDOTilesDataset(\n",
    "    data_path=DATA_PATH, augmentation_list=augmentation_list, augmentation_strategy='single', data_stride=DATA_STRIDE\n",
    ")\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc692f1",
   "metadata": {},
   "source": [
    "### Visualize Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random index\n",
    "idx = np.random.randint(0, high=dataset.__len__())\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1, _ = dataset.__getitem__(idx)\n",
    "\n",
    "fig = plt.figure(figsize=np.array([4, 2]), constrained_layout=True)\n",
    "spec = fig.add_gridspec(ncols=2, nrows=1, wspace=0, hspace=0)\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "ax.imshow(rearrange(x0, 'c h w -> h w c'))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Original\")\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 1])\n",
    "ax.imshow(rearrange(x1, 'c h w -> h w c'))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Augmented\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81b8cd-b654-47ae-9657-69b3259958e6",
   "metadata": {},
   "source": [
    "# Setup training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8d9f7-365f-418a-86e2-d0601fa1fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 64\n",
    "AUGMENTATION = 'single'\n",
    "LOSS = 'contrast'   # 'contrast' or 'cos'\n",
    "LEARNING_RATE = 0.1\n",
    "PROJECTION_HEAD_SIZE = 128\n",
    "PREDICTION_HEAD_SIZE = 128\n",
    "EMBEDING_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d115108-1a4a-47f3-914d-30acdec80cbe",
   "metadata": {},
   "source": [
    "# Build dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac07cf-9410-4ad5-8713-4c6637a750a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b7e371-193d-4113-a0f2-71223f697eaa",
   "metadata": {},
   "source": [
    "# Setup SimSiam model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7979d6-7d91-4e6c-b30f-27cb780714eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.projection_head = SimSiamProjectionHead(512, 512, PROJECTION_HEAD_SIZE)\n",
    "        self.prediction_head = SimSiamPredictionHead(PROJECTION_HEAD_SIZE, EMBEDING_SIZE, PREDICTION_HEAD_SIZE)\n",
    "        self.criterion = NegativeCosineSimilarity()\n",
    "\n",
    "        self.loss = LOSS\n",
    "        self.loss_cos = NegativeCosineSimilarity()\n",
    "        self.loss_contrast = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1, _) = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "\n",
    "        loss_cos = 0.5 * (self.loss_cos(p0, z1) + self.loss_cos(p1, z0))\n",
    "        loss_contrast = 0.5 * (self.loss_contrast(p0, z1) + self.loss_contrast(p1, z0))\n",
    "\n",
    "        if self.loss == 'cos':\n",
    "            loss = loss_cos\n",
    "        else:\n",
    "            loss = loss_contrast\n",
    "\n",
    "        self.log('loss cos', loss_cos)\n",
    "        self.log('loss contrast', loss_contrast)\n",
    "        self.log('loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n",
    "        return optim\n",
    "        \n",
    "model = SimSiam()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be944f-e4f8-45d1-84d8-12027fe4d138",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3590ea0-8087-41c0-8382-a072681c25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS,\n",
    "                     accelerator=DEVICE, devices=1, strategy=\"auto\",deterministic=True)\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b528f",
   "metadata": {},
   "source": [
    "# Visualize Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bd98e-7676-44f8-97e6-9761358bb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the model is trained, embed images into dataset\n",
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "# disable gradients for faster calculations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # passes batches and filenames to model to find embeddings\n",
    "    # embedding -> vectorize image, simpler representation of image\n",
    "    for i, (x, _, fnames) in enumerate(val_dataloader):\n",
    "        # move the images to the gpu\n",
    "        # x = x.to(DEVICE)\n",
    "        # embed the images with the pre-trained backbone\n",
    "        y = model.backbone(x).flatten(start_dim=1)\n",
    "        # store the embeddings and filenames in lists\n",
    "        embeddings.append(y)\n",
    "        filenames = filenames + list(fnames)\n",
    "\n",
    "# concatenate the embeddings and convert to numpy\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=5\n",
    "min_dist=0.0\n",
    "n_components=2\n",
    "metric='euclidean'\n",
    "spread = 0.5\n",
    "repulsion_strength = 2\n",
    "\n",
    "fit = UMAP(\n",
    "    n_neighbors=n_neighbors,\n",
    "    # min_dist=min_dist,\n",
    "    # n_components=n_components,\n",
    "    metric=metric,\n",
    "    # spread=spread,\n",
    "    # repulsion_strength=repulsion_strength,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "embeddings_2d = fit.fit_transform(embeddings)\n",
    "# normalize the embeddings to fit in the [0, 1] square\n",
    "M = np.max(embeddings_2d, axis=0)\n",
    "m = np.min(embeddings_2d, axis=0)\n",
    "embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a51b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for the scatter plot we want to transform the images to a two-dimensional\n",
    "# # vector space using a random Gaussian projection\n",
    "# projection = random_projection.GaussianRandomProjection(n_components=2)\n",
    "# embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "# # normalize the embeddings to fit in the [0, 1] square\n",
    "# M = np.max(embeddings_2d, axis=0)\n",
    "# m = np.min(embeddings_2d, axis=0)\n",
    "# embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ab12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a scatter plot of the dataset\n",
    "# clustering similar images together\n",
    "\n",
    "def get_scatter_plot_with_thumbnails():\n",
    "    \"\"\"Creates a scatter plot with image overlays.\"\"\"\n",
    "    # initialize empty figure and add subplot\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(\"Scatter Plot of the SDO/AIA 171 Tiles\")\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # shuffle images and find out which images to show\n",
    "    shown_images_idx = []\n",
    "    shown_images = np.array([[1.0, 1.0]])\n",
    "    iterator = [i for i in range(embeddings_2d.shape[0])]\n",
    "    np.random.shuffle(iterator)\n",
    "    for i in iterator:\n",
    "        # only show image if it is sufficiently far away from the others\n",
    "        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 2e-3:\n",
    "            continue\n",
    "        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n",
    "        shown_images_idx.append(i)\n",
    "\n",
    "    # plot image overlays\n",
    "    for idx in shown_images_idx:\n",
    "        thumbnail_size = int(rcp[\"figure.figsize\"][0] * 2.0)\n",
    "        # path = os.path.join(path_to_data, filenames[idx])\n",
    "        img = Image.open(filenames[idx])\n",
    "        img = functional.resize(img, thumbnail_size)\n",
    "        img = np.array(img)\n",
    "        img_box = osb.AnnotationBbox(\n",
    "            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n",
    "            embeddings_2d[idx],\n",
    "            pad=0.2,\n",
    "        )\n",
    "        ax.add_artist(img_box)\n",
    "\n",
    "    # set aspect ratio\n",
    "    ratio = 1.0 / ax.get_data_ratio()\n",
    "    ax.set_aspect(ratio, adjustable=\"box\")\n",
    "\n",
    "\n",
    "# get a scatter plot with thumbnail overlays\n",
    "get_scatter_plot_with_thumbnails()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
