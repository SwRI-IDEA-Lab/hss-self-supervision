{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2572238e-27d0-4e97-8bb4-de5f5bb9bd23",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ce1e6-0485-4cae-8fdf-163b6a2812d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from lightly.loss import NegativeCosineSimilarity, NTXentLoss\n",
    "from lightly.models.modules.heads import SimSiamPredictionHead, SimSiamProjectionHead\n",
    "\n",
    "from sklearn import random_projection\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "import matplotlib.offsetbox as osb\n",
    "from matplotlib import rcParams as rcp\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from cuml import UMAP\n",
    "from cuml.cluster import hdbscan\n",
    "\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "from data.dataset import SDOTilesDataset\n",
    "from data.augmentation_list import AugmentationList\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.image_utils import read_image\n",
    "\n",
    "seed = 42  # So clever.\n",
    "pl.seed_everything(seed, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296a6ea",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8c1b6",
   "metadata": {},
   "source": [
    "### Define augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a343179",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list = AugmentationList('euv')\n",
    "augmentation_list.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list.keys = ['h_flip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54742957",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78347940",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list.randomize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d90cbb-f8d9-4af7-aadb-ceebfc1c937b",
   "metadata": {},
   "source": [
    "### Initialize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489a6b9-5603-4df7-9c60-0dd399870b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/home/jovyan/scratch_space/andresmj/data/AIA_211_193_171_256x256_small'\n",
    "# DATA_PATH = '/d0/euv/aia/preprocessed_ext/AIA_211_193_171/AIA_211_193_171_256x256'\n",
    "DATA_PATH = '/d0/euv/aia/preprocessed_ext/AIA_211_193_171/AIA_211_193_171_256x256_small'\n",
    "DATA_STRIDE = 10\n",
    "dataset = SDOTilesDataset(\n",
    "    data_path=DATA_PATH, augmentation_list=augmentation_list, augmentation_strategy='single', data_stride=DATA_STRIDE\n",
    ")\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417fae7-5a13-4510-aaae-af6cb666e119",
   "metadata": {},
   "source": [
    "### Visualize Solar image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7002ec1-b1be-446e-b311-7340c829c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jovyan/scratch_space/andresmj/data/20141109_080002_aia_211_193_171.jpg'\n",
    "img = mpimg.imread(image_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc692f1",
   "metadata": {},
   "source": [
    "### Visualize Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random index\n",
    "idx = np.random.randint(0, high=dataset.__len__())\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1, _ = dataset.__getitem__(idx)\n",
    "\n",
    "fig = plt.figure(figsize=np.array([4, 2]), constrained_layout=True)\n",
    "spec = fig.add_gridspec(ncols=2, nrows=1, wspace=0, hspace=0)\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "ax.imshow(rearrange(x0, 'c h w -> h w c'))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Original\")\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 1])\n",
    "ax.imshow(rearrange(x1, 'c h w -> h w c'))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Augmented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8d9f7-365f-418a-86e2-d0601fa1fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 256\n",
    "AUGMENTATION = 'single'\n",
    "LOSS = 'contrast'   # 'contrast' or 'cos'\n",
    "LEARNING_RATE = 0.1\n",
    "PROJECTION_HEAD_SIZE = 128\n",
    "PREDICTION_HEAD_SIZE = 128\n",
    "EMBEDING_SIZE = 64\n",
    "\n",
    "CHECKPOINT_PATH = '/d0/amunozj/git_repos/hss-self-supervision/sim_siam_256_ext/epoch-epoch=15.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d115108-1a4a-47f3-914d-30acdec80cbe",
   "metadata": {},
   "source": [
    "# Build dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac07cf-9410-4ad5-8713-4c6637a750a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b7e371-193d-4113-a0f2-71223f697eaa",
   "metadata": {},
   "source": [
    "# Setup SimSiam model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7979d6-7d91-4e6c-b30f-27cb780714eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.projection_head = SimSiamProjectionHead(512, 512, PROJECTION_HEAD_SIZE)\n",
    "        self.prediction_head = SimSiamPredictionHead(PROJECTION_HEAD_SIZE, EMBEDING_SIZE, PREDICTION_HEAD_SIZE)\n",
    "        self.criterion = NegativeCosineSimilarity()\n",
    "\n",
    "        self.loss = LOSS\n",
    "        self.loss_cos = NegativeCosineSimilarity()\n",
    "        self.loss_contrast = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1, _) = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "\n",
    "        loss_cos = 0.5 * (self.loss_cos(p0, z1) + self.loss_cos(p1, z0))\n",
    "        loss_contrast = 0.5 * (self.loss_contrast(p0, z1) + self.loss_contrast(p1, z0))\n",
    "\n",
    "        if self.loss == 'cos':\n",
    "            loss = loss_cos\n",
    "        else:\n",
    "            loss = loss_contrast\n",
    "\n",
    "        self.log('loss cos', loss_cos)\n",
    "        self.log('loss contrast', loss_contrast)\n",
    "        self.log('loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n",
    "        return optim\n",
    "        \n",
    "model = SimSiam.load_from_checkpoint(CHECKPOINT_PATH).to(DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b528f",
   "metadata": {},
   "source": [
    "# Visualize Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bd98e-7676-44f8-97e6-9761358bb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the model is trained, embed images into dataset\n",
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "# disable gradients for faster calculations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # passes batches and filenames to model to find embeddings\n",
    "    # embedding -> vectorize image, simpler representation of image\n",
    "    for i, (x, _, fnames) in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "        # move the images to the gpu\n",
    "        # x = x.to(DEVICE)\n",
    "        # embed the images with the pre-trained backbone\n",
    "        y = model.backbone(x.to(DEVICE)).flatten(start_dim=1)\n",
    "        # store the embeddings and filenames in lists\n",
    "        embeddings.append(y)\n",
    "        filenames = filenames + list(fnames)\n",
    "\n",
    "# concatenate the embeddings and convert to numpy\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=5\n",
    "min_dist=0.0\n",
    "n_components=2\n",
    "metric='euclidean'\n",
    "spread = 0.5\n",
    "repulsion_strength = 2\n",
    "\n",
    "fit = UMAP(\n",
    "    n_neighbors=n_neighbors,\n",
    "    # min_dist=min_dist,\n",
    "    # n_components=n_components,\n",
    "    metric=metric,\n",
    "    # spread=spread,\n",
    "    # repulsion_strength=repulsion_strength,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "embeddings_2d = fit.fit_transform(embeddings)\n",
    "# normalize the embeddings to fit in the [0, 1] square\n",
    "M = np.max(embeddings_2d, axis=0)\n",
    "m = np.min(embeddings_2d, axis=0)\n",
    "embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a51b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for the scatter plot we want to transform the images to a two-dimensional\n",
    "# # vector space using a random Gaussian projection\n",
    "# projection = random_projection.GaussianRandomProjection(n_components=2)\n",
    "# embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "# # normalize the embeddings to fit in the [0, 1] square\n",
    "# M = np.max(embeddings_2d, axis=0)\n",
    "# m = np.min(embeddings_2d, axis=0)\n",
    "# embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ab12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a scatter plot of the dataset\n",
    "# clustering similar images together\n",
    "\n",
    "def get_scatter_plot_with_thumbnails():\n",
    "    \"\"\"Creates a scatter plot with image overlays.\"\"\"\n",
    "    # initialize empty figure and add subplot\n",
    "    fig = plt.figure(figsize=(9,9), dpi=150)\n",
    "    fig.suptitle(\"Scatter Plot of the SDO/AIA 171 Tiles\")\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # shuffle images and find out which images to show\n",
    "    shown_images_idx = []\n",
    "    shown_images = np.array([[1.0, 1.0]])\n",
    "    iterator = [i for i in range(embeddings_2d.shape[0])]\n",
    "    np.random.shuffle(iterator)\n",
    "    for i in iterator:\n",
    "        # only show image if it is sufficiently far away from the others\n",
    "        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 5e-4:\n",
    "            continue\n",
    "        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n",
    "        shown_images_idx.append(i)\n",
    "\n",
    "    # plot image overlays\n",
    "    for idx in shown_images_idx:\n",
    "        thumbnail_size = int(rcp[\"figure.figsize\"][0] * 2.0)\n",
    "        # path = os.path.join(path_to_data, filenames[idx])\n",
    "        img = Image.open(filenames[idx])\n",
    "        img = functional.resize(img, thumbnail_size)\n",
    "        img = np.array(img)\n",
    "        img_box = osb.AnnotationBbox(\n",
    "            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n",
    "            embeddings_2d[idx],\n",
    "            pad=0.2,\n",
    "        )\n",
    "        ax.add_artist(img_box)\n",
    "\n",
    "    # set aspect ratio\n",
    "    ratio = 1.0 / ax.get_data_ratio()\n",
    "    ax.set_aspect(ratio, adjustable=\"box\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "# get a scatter plot with thumbnail overlays\n",
    "ratio = get_scatter_plot_with_thumbnails()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7bb95",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f82f2-8586-4c19-98bc-099a2b221d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 5\n",
    "min_cluster_size = 5\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size)\n",
    "clusterer.fit(embeddings_2d)\n",
    "\n",
    "# Plotting best results\n",
    "sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
    "\n",
    "color_palette = sns.color_palette('Paired', clusterer.labels_.max()+1)\n",
    "cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
    "cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
    "cluster_alphas = np.ones_like(clusterer.labels_)*0.5\n",
    "cluster_alphas[clusterer.labels_==-1] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948df462",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER_SIZE = 3\n",
    "fig = plt.figure(figsize=(9,9), dpi=150)\n",
    "u = embeddings_2d\n",
    "if n_components == 1:\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(u[:,0], range(len(u)), c=cluster_colors, s=MARKER_SIZE, alpha=cluster_alphas)\n",
    "if n_components == 2:\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(u[:,0], u[:,1], c=cluster_colors, s=MARKER_SIZE, alpha=cluster_alphas)\n",
    "if n_components == 3:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(u[:,0], u[:,1], u[:,2], c=cluster_colors, s=MARKER_SIZE, alpha=cluster_alphas)\n",
    "ax.set_aspect(ratio, adjustable=\"box\")\n",
    "# plt.title(title, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d093d",
   "metadata": {},
   "source": [
    "# Visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46871554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_plot(cluster_rows:int, cluster_col:int, images_list:np.array, dpi:int):\n",
    "  fig = plt.figure(figsize=[cluster_col, cluster_rows], layout='constrained', dpi=dpi)\n",
    "  spec = gridspec.GridSpec(ncols=cluster_col, nrows=cluster_rows, figure=fig, wspace=0, hspace=0)\n",
    "\n",
    "  # Shuffle list and use first 16 filepaths to plot images\n",
    "  np.random.shuffle(images_list)\n",
    "\n",
    "  # For loop to go through and use Team Yellow's load image module\n",
    "  n = 0\n",
    "  for j in range(cluster_rows):\n",
    "    for i in range(cluster_col):\n",
    "      if images_list.shape[0] > n:\n",
    "        image = read_image(image_loc = images_list[n], image_format = \"jpg\")\n",
    "        # Scatter plot\n",
    "        ax1 = fig.add_subplot(spec[j, i])\n",
    "        ax1.imshow(image)\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "      else:\n",
    "        break\n",
    "      n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd047868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all cluster labels\n",
    "n_clusters_2_plot = 20\n",
    "\n",
    "# Create a random draw of integers between a range\n",
    "clusters_2_plot = np.random.choice(clusterer.labels_[clusterer.labels_ >= 0],\n",
    "                                   size=n_clusters_2_plot,\n",
    "                                   replace=False)\n",
    "\n",
    "for cluster in clusters_2_plot:\n",
    "  cluster_plot(cluster_rows=2,\n",
    "               cluster_col=10,\n",
    "               images_list=np.array(filenames)[clusterer.labels_==cluster],\n",
    "               dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
